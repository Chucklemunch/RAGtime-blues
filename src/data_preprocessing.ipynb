{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ca3771-e64b-4a17-a6fc-493efb8784ea",
   "metadata": {},
   "source": [
    "# Gets from Pub Med and process it so it can be chunked, vectorized, and stored in Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eef88f93-0052-4fbf-ae6f-31eacda94f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from lxml import etree\n",
    "from io import BytesIO\n",
    "import re\n",
    "import spacy\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923491be-3caa-45d9-97c7-13098d796b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uid': '12788749',\n",
       "  'pmcid': 'PMC12788749',\n",
       "  'title': 'comparisons_of_the_radiological_and_functional_results_of_femoral_tunnels_created_in_figure_4_and_110_flexion_positions_in_arthroscopic_anterior_cruciate_ligament_reconstruction'},\n",
       " {'uid': '12788292',\n",
       "  'pmcid': 'PMC12788292',\n",
       "  'title': 'assessment_and_measurement_of_the_side_effects_of_an_evidence_based_intervention_with_an_advanced_smart_cricket_ball_exemplified_by_a_case_report_on_correcting_illegal_bowling_action'},\n",
       " {'uid': '12788198',\n",
       "  'pmcid': 'PMC12788198',\n",
       "  'title': 'impact_of_anatomical_placement_on_the_accuracy_of_wearable_heart_rate_monitors_during_rest_and_various_exercise_intensities'},\n",
       " {'uid': '12788087',\n",
       "  'pmcid': 'PMC12788087',\n",
       "  'title': 'from_laboratory_to_field_concurrent_validity_of_kinovea_s_linear_kinematics_tracking_tool_for_semi_automated_countermovement_jump_analysis'},\n",
       " {'uid': '12787239',\n",
       "  'pmcid': 'PMC12787239',\n",
       "  'title': 'clinical_outcomes_of_arthroscopic_treatment_for_triangular_fibrocartilage_complex_lesions_in_adolescent_elite_athletes'},\n",
       " {'uid': '12787227',\n",
       "  'pmcid': 'PMC12787227',\n",
       "  'title': 'differing_definitions_of_outpatient_surgery_may_influence_study_outcomes_related_to_acl_reconstruction'},\n",
       " {'uid': '12787026',\n",
       "  'pmcid': 'PMC12787026',\n",
       "  'title': 'shoulder_instability_in_the_u_s_military_a_systematic_review_of_epidemiology_operative_management_and_outcomes'},\n",
       " {'uid': '12786909',\n",
       "  'pmcid': 'PMC12786909',\n",
       "  'title': 'lifestyle_and_selected_issues_related_to_sexual_health_the_importance_of_specialist_care_in_balneology_dietetics_and_physiotherapy'},\n",
       " {'uid': '12786798',\n",
       "  'pmcid': 'PMC12786798',\n",
       "  'title': 'reframing_ankle_sprain_management_the_role_of_thermography_in_ligament_injury_monitoring'},\n",
       " {'uid': '12786792',\n",
       "  'pmcid': 'PMC12786792',\n",
       "  'title': 'a_systematic_review_of_rehabilitation_interventions_for_athletes_with_chronic_ankle_instability'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Entrez.email = \"charlie.kotula@gmail.com\"\n",
    "\n",
    "# Search query for getting relevant research articles\n",
    "query = \"\"\"\n",
    "(\n",
    "  rehabilitation AND \"physical therapy\" OR \"return to sport\" OR \"return to play\"\n",
    ") AND (\n",
    "  injury OR surgery OR postoperative OR musculoskeletal\n",
    ") AND (\n",
    "  exercise OR \"therapeutic exercise\" OR training\n",
    ") AND (\n",
    "  review[pt] OR systematic review[pt] OR meta-analysis[pt]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "def get_ids_with_metadata(query):\n",
    "    \"\"\"\n",
    "    Gets PMC article UIDs, PMCIDs, and titles based on search query\n",
    "\n",
    "    Args: query (str) - search query used to retrieve PMC articles\n",
    "    Returns: metadata (dict) - dictionary containing PMCID and titles corresponding\n",
    "        to the articles UID\n",
    "    \"\"\"\n",
    "    # Get relevant UIDs and titles\n",
    "    metadata = []\n",
    "    handle = Entrez.esearch(\n",
    "        db='pmc',\n",
    "        term=query,\n",
    "        retmax=10, # CHANGE\n",
    "    )\n",
    "    \n",
    "    # Get relevant articles\n",
    "    uids = Entrez.read(handle)['IdList']\n",
    "    handle.close()\n",
    "    \n",
    "    # Get summaries for metadata\n",
    "    summary = Entrez.esummary(\n",
    "        db='pmc',\n",
    "        id=','.join(uids)\n",
    "    )\n",
    "    records = Entrez.read(summary)\n",
    "    \n",
    "    # Map UIDs to titles and pmids\n",
    "    for rec in records:\n",
    "        title = rec['Title'].lower()\n",
    "        title = re.sub(r'[^a-z0-9]+', '_', title)\n",
    "    \n",
    "        metadata.append(\n",
    "            {\n",
    "                'uid': rec['Id'],\n",
    "                'pmcid': rec['ArticleIds']['pmcid'],\n",
    "                'title': title\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return metadata\n",
    "\n",
    "# Create metadata list to be used in multiprocessing\n",
    "metadata = get_ids_with_metadata(query)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa04efa1-4ffe-4490-ab31-8c59b3a7646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Functions to extract, clean, and chunk text from PMC\n",
    "def get_xml(pmc_id):\n",
    "    \"\"\"\n",
    "    Returns the xml tree representation of the PMC article corresponding to\n",
    "    the input UID.\n",
    "    \"\"\"\n",
    "    handle = Entrez.efetch(\n",
    "        db='pmc',\n",
    "        id=pmc_id,\n",
    "        retmode='xml',\n",
    "        # rettype='full'\n",
    "    )\n",
    "\n",
    "    xml_dat = handle.read()\n",
    "\n",
    "    # Converts xml bytes to tree\n",
    "    xml_tree = etree.parse(BytesIO(xml_dat))\n",
    "    \n",
    "    return xml_tree\n",
    "\n",
    "def get_text(xml):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of {section title: content} for the xml tree root.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "\n",
    "    root = xml.getroot()\n",
    "\n",
    "    # Remove references\n",
    "    for xref in root.xpath('.//xref'):\n",
    "        parent = xref.getparent()\n",
    "        if parent is None:\n",
    "            continue\n",
    "\n",
    "        # removes punction surrounding references\n",
    "        prev = xref.getprevious()\n",
    "\n",
    "        # Handles punctuation before ref\n",
    "        if prev is not None and prev.tail is not None:\n",
    "            prev.tail = re.sub(r'[\\[\\(]\\s*$', ' ', prev.tail)\n",
    "        else:\n",
    "            # xref is the first child â†’ clean parent.text\n",
    "            if parent.text:\n",
    "                parent.text = re.sub(r'[\\[\\(]\\s*$', ' ', parent.text)   \n",
    "\n",
    "        # Handles punctuation after ref\n",
    "        if xref.tail:\n",
    "            xref.tail = re.sub(r'^\\s*[\\]\\)]*', ' ', xref.tail)\n",
    "            \n",
    "        parent.remove(xref)\n",
    "            \n",
    "    \n",
    "    for sec in root.xpath('.//body//sec'):\n",
    "        title = sec.findtext('title')\n",
    "        if not title:\n",
    "            continue\n",
    "        title = title.lower()\n",
    "\n",
    "        # Gets paragraphs from each section\n",
    "        paragraphs = [\n",
    "            ''.join(p.itertext()) for p in sec.findall('p')\n",
    "        ]\n",
    "    \n",
    "        # Add sections to sections list\n",
    "        if paragraphs: # ignores empty sections\n",
    "            text.append((title , ' '.join(paragraph for paragraph in paragraphs)))\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans article text, removing extra spaces, etc.\n",
    "    \"\"\"\n",
    "    cleaned_text = []\n",
    "    for section, words in text:\n",
    "        words = re.sub(r'\\s+', ' ', words)\n",
    "        words = words.replace('\\xa0', ' ').strip()\n",
    "        cleaned_text.append((section, words))\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def chunk_text(cleaned_text, uid, pmcid, title,):\n",
    "    \"\"\"\n",
    "    Takes cleaned article text and chunks it into LangChain Documents\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    for section, text in cleaned_text:\n",
    "        # Create section label for metadata\n",
    "        section = section.lower()\n",
    "        section = re.sub(r'[^a-z0-9]+', '_', section)\n",
    "    \n",
    "        # Chunk text\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            separators=['\\n\\n', '\\n', '. ', ' ', '']\n",
    "        )\n",
    "        chunks = splitter.split_text(text)\n",
    "    \n",
    "        # Create langchain Documents\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc=Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"uid\": uid,\n",
    "                    \"pmcid\": pmcid,\n",
    "                    \"article\": title,\n",
    "                    \"section\": section,\n",
    "                    \"chunk_id\": f'{uid}-{section}-{i}'\n",
    "                }\n",
    "            )\n",
    "\n",
    "            docs.append(doc)\n",
    "\n",
    "    return docs\n",
    "\n",
    "def process_article(article):\n",
    "    # get metadata for Document creation\n",
    "    uid = article['uid']\n",
    "    pmcid = article['pmcid']\n",
    "    title = article['title']\n",
    "    \n",
    "    # extract sections\n",
    "    xml = get_xml(uid)\n",
    "    text = get_text(xml)\n",
    "\n",
    "    # clean sections\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # chunk text and create LangChain Documents with metadata\n",
    "    docs = chunk_text(cleaned_text, uid, pmcid, title)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e854873-fc28-4ef7-a355-c19f702269b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data_preprocessing import process_article\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #### Multiprocessing of articles ####\n",
    "    documents = []\n",
    "    \n",
    "    ### Processes articles one at a time\n",
    "    # for article in tqdm(metadata):\n",
    "    #     # process article (extract text, clean, chunk)d\n",
    "    #     docs = process_article(article)\n",
    "    #     documents.append(docs)\n",
    "    \n",
    "    ### Processes multiple articles in parallel\n",
    "    with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        futures = [executor.submit(process_article, article) for article in metadata]\n",
    "    \n",
    "        for future in as_completed(futures):\n",
    "            documents.extend(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e4c78-9bcc-4688-8921-7f767366cd39",
   "metadata": {},
   "source": [
    "# Embedding using OpenAI and Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132074ec-9e06-49ce-9b4a-5b0b707ad8b1",
   "metadata": {},
   "source": [
    "### To run the Qdrant docker:\n",
    "\n",
    "`docker run -p 6333:6333 -p 6334:6334 \\\n",
    "    -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
    "    qdrant/qdrant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f7eb2-ae3e-4f8a-8704-d843b0590971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e04ba3-3dca-4632-97de-adf7ba7839c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"rehab_collection\",\n",
    "    vectors_config=VectorParams(\n",
    "        size=3072,\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444c76f-2faf-4fd9-93d6-dc001eb784ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae3c4f-3c31-4f09-a358-2841edf4dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in documents]\n",
    "metadatas = [doc.metadata for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d16d2-ba59-4432-b6a5-2a0421d492db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376d5d6-aa83-4b1a-b934-4f7d91754426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create points for vector db\n",
    "points = []\n",
    "\n",
    "for vec, doc in zip(vectors, documents):\n",
    "    point_id = uuid.uuid5(uuid.NAMESPACE_DNS, doc.metadata['chunk_id'])\n",
    "    \n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=point_id,\n",
    "            vector=vec,\n",
    "            payload={\n",
    "                **doc.metadata,\n",
    "                'text': doc.page_content\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d56668-51a4-45a6-b01c-5b7733938fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "points[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945fc6b-c0b0-44e9-bb82-737b023921ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsert to db in batches\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def batched(points, batch_size):\n",
    "    for i in range(0, len(points), batch_size):\n",
    "        yield points[i : i + batch_size]\n",
    "\n",
    "for point_batch in batched(points, BATCH_SIZE):\n",
    "    operation_info = client.upsert(\n",
    "        collection_name=\"rehab_collection\",\n",
    "        wait=True,\n",
    "        points=point_batch\n",
    "    )\n",
    "    \n",
    "    print(operation_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268a86e-bb9a-4b9a-a182-b55cc59cd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking db\n",
    "client.count(collection_name='rehab_collection', exact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847db8c5-af73-4802-b95f-79cd2a44a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wipe db\n",
    "client.delete_collection('rehab_collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ecb528-a028-485a-9905-e7b4e253c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing search\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
